/mnt/msranlp/shumma/data/16g/
--save-dir
/home/v-jiaya/unilm-moe/data/PretrainedModels/electra-encoder-decoder-v6/lr5e-4-bsz8192-ws10000-wd0.01-dw10_1000_-1-iw1.0_1000_-1-g12d4--share-generator-discriminator/
--tensorboard-logdir
/home/v-jiaya/unilm-moe/data/PretrainedModels/electra-encoder-decoder-v6/logs
--arch
electra_encoder_decoder_v6_base
--criterion
electra_encoder_decoder_v6
--task
pretraining
--tokens-per-sample
512
--mask-prob
0.15
--span-length
3.0
--leave-unmasked-prob
0.0
--random-token-prob
0.0
--spm-model
/mnt/msranlp/shumma/data/16g/sentencepiece.bpe.model
--dict-file
/mnt/msranlp/shumma/data/16g/128k/dict.txt
--share-all-embeddings
--required-batch-size-multiple
8
--max-source-positions
1024
--max-target-positions
1024
--optimizer
adam
--adam-betas
(0.9,0.98)
--lr-scheduler
inverse_sqrt
--lr
2e-4
--warmup-init-lr
1e-07
--stop-min-lr
1e-09
--warmup-updates
4000
--max-update
1000000
--max-epoch
100
--disable-validation
--save-interval-updates
5000
--no-epoch-checkpoints
--max-sentences
2
--update-freq
1
--clip-norm
2.0
--seed
1
--log-format
simple
--log-interval
1
--skip-invalid-size-inputs-valid-test
--batch-read-ahead
10
--rel-pos-buckets
32
--max-rel-pos
128
--rescale-init
--encoder-layers
1
--generator-decoder-layers
2
--discriminator-decoder-layers
1
--share-generator-discriminator
--log-file
/home/v-jiaya/unilm-moe/data/PretrainedModels/electra-encoder-decoder-v6/train.log
--ddp-backend=no_c10d
--debug