description: ft
target:
  service: amlk8s
  name: a100-8x-wus2
  vc: quantus
  #name: itpwus2v100cl
  #vc: gcrprojvc1
  #name: v100-8x-eus-1
  #vc: quantus
  #name: itplabrr1cl1     
  #vc: resrchvc
  #name: v100-8x-scus     
  #vc: quantus
  #name: v100-8x-eus-1    
  #vc: quantus

environment:
  image: nvidia/20.09:v7.0.2
  registry: shumingdocker.azurecr.io
  setup:
  - pwd
  - ls ~/
  - ls ./
  - ulimit -n 40960
  - pip install --user fairseq/
  - pip install --user infinibatch/
  - pip install -U git+https://github.com/pltrdy/pyrouge --user 
  - git clone https://github.com/pltrdy/files2rouge.git && cd files2rouge && echo '/tmp/code/.files2rouge/' | python setup_rouge.py && python setup.py install --user
  - python -m nltk.downloader all > nltk.log
  - echo "master_addr:" "$$MASTER_ADDR"
  - echo "master_port:" $$MASTER_PORT
  - echo "node_rank:" $$OMPI_COMM_WORLD_RANK
  username: shumingdocker

code:
  local_dir: $CONFIG_DIR/../../..

storage:
  msranlp:
    storage_account_name: msranlp
    container_name: unilm
  output:
    storage_account_name: yangjianunilm
    container_name: unilm

search:
  job_template:
    name: ft_wmt14_fr2en_{initialization_strategy}-{branch}-{ckpt}-gpus8-uf{update_freq}-bs{batch_size}-lr{lr}-ws{warmup_steps}-sd{seed}-cn{clip_norm}-wd{weight_decay}
    sku: G8
    sku_count: 1
    command:
    - export PRETRAINED_MODEL_PATH=/mnt/output/PretrainedModels/multilingual/electra-encoder-decoder-v6/{branch}/{ckpt}.pt
    - export DATA_DIR=/mnt/output/wmt14-en-fr/data-bin/
    - export OUTPUT_DIR=/mnt/output/wmt14-en-fr/model/electra-encoder-decoder-v6/{branch}/{ckpt}-ft/fr2en/lr{lr}-gpus8-uf{update_freq}-bs{batch_size}-sd{seed}-ws{warmup_steps}-cn{clip_norm}-wd{weight_decay}-{src_maxlen}_{tgt_maxlen}/{initialization_strategy}/
    - bash ./shells/mt/wmt14-en-fr/finetune_wmt14_en_fr_electra_nlg.sh $${DATA_DIR} $${PRETRAINED_MODEL_PATH} $${OUTPUT_DIR} electra_encoder_decoder_v6_base {update_freq} {batch_size} {lr} {warmup_steps} {seed} {max_epoch} {clip_norm} {weight_decay} {src_maxlen} {tgt_maxlen} fr en "--initialization-strategy {initialization_strategy} --generator-decoder-layers 12"
    - bash ./shells/mt/wmt14-en-fr/evaluate_wmt14_en_fr.sh $${OUTPUT_DIR} checkpoint_best.pt 16 {beam} {lenpen} {tgt_minlen} {tgt_maxlen} {split} fr en
    submit_args:
      env:
        NCCL_DEBUG: INFO
        MKL_NUM_THREADS: 1
        OMP_NUM_THREADS: 1
  type: grid
  max_trials: 500
  params:
    - name: branch
      spec: discrete
      values: ["lr3e-4-bsz8192-ws10000-wd0.05-dw10_-1_-1-iw1.0_-1_-1-g12d4-125K"]
    - name: ckpt
      spec: discrete
      values: ["checkpoint_1_125000"]
    - name: seed
      spec: discrete
      values: [1]
    - name: lr
      spec: discrete
      values: ["1e-4"]
    - name: update_freq
      spec: discrete
      values: [64]
    - name: batch_size
      spec: discrete
      values: [1024]
    - name: max_epoch
      spec: discrete
      values: [50]
    - name: warmup_steps
      spec: discrete
      values: [4000]
    - name: initialization_strategy
      spec: discrete
      values: ["generator"]
    - name: beam
      spec: discrete
      values: [8]
    - name: lenpen
      spec: discrete
      values: [1.0]
    - name: src_maxlen
      spec: discrete
      values: [1024]
    - name: tgt_minlen
      spec: discrete
      values: [0]
    - name: tgt_maxlen
      spec: discrete
      values: [1024]
    - name: clip_norm
      spec: discrete
      values: [0.0]
    - name: weight_decay
      spec: discrete
      values: [0.0]
    - name: split
      spec: discrete
      values: ["test"]